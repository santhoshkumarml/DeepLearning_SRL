{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "require 'torch';\n",
    "require 'nn';\n",
    "\n",
    "START = \"$START$\"\n",
    "FINISH = \"$END$\"\n",
    "FILE_PATH = \"dataset/language_model.data\"\n",
    "WORD_VEC_SIZE = 25\n",
    "WINDOW_SIZE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "-- initialize Random Word Vec\n",
    "function initWordVec()\n",
    "    return torch.randn(WORD_VEC_SIZE)\n",
    "end\n",
    "\n",
    "-- read and Store Data\n",
    "function readFile(file_path)\n",
    "    local f = io.open(file_path)\n",
    "    local store = {}\n",
    "    local word_dict = {}\n",
    "    word_dict[START] = initWordVec()\n",
    "    word_dict[FINISH] = initWordVec()\n",
    "    while true do\n",
    "        local l = f:read()\n",
    "        if not l then break end\n",
    "        local words = {}\n",
    "        table.insert(words, START)\n",
    "        for word in l:gmatch(\"%w+\") do \n",
    "            table.insert(words, word)\n",
    "            if not word_dict[word] then\n",
    "                word_dict[word] = initWordVec()\n",
    "            end\n",
    "        end\n",
    "        table.insert(words,FINISH)\n",
    "        table.insert(store, words)\n",
    "    end\n",
    "    return store, word_dict\n",
    "end\n",
    "\n",
    "-- split into windows and make training data\n",
    "function makeTrainingData(store, window_size, word_dict)\n",
    "    local train_data = {}\n",
    "    for idx, line in ipairs(store) do\n",
    "        local size = table.getn(line)\n",
    "        local no_of_windows = math.max( 0, size - window_size) + 1\n",
    "\n",
    "        for widx = 1, no_of_windows do\n",
    "            local training_inst = {}\n",
    "            local data = {}\n",
    "            local word_idx = widx\n",
    "\n",
    "            --Add to data the words for the window\n",
    "            while table.getn(data) < window_size and word_idx < size do\n",
    "                table.insert(data, line[word_idx])\n",
    "                word_idx = word_idx + 1\n",
    "            end\n",
    "                \n",
    "            --Align to Window Size\n",
    "            while word_idx >= size and table.getn(data) < window_size do\n",
    "                table.insert(data, FINISH)\n",
    "                word_idx = word_idx + 1\n",
    "            end\n",
    "            -- Get WordVector From Dictionary\n",
    "            local word_vec_data = word_dict[data[1]]\n",
    "\n",
    "            for i = 2, table.getn(data) do\n",
    "                word_vec_data = torch.cat(word_vec_data, word_dict[data[i]])\n",
    "            end\n",
    "            \n",
    "            training_inst.data = word_vec_data\n",
    "            training_inst.label = 1\n",
    "            table.insert(train_data, training_inst)\n",
    "        end   \n",
    "    end\n",
    "    return train_data\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function construct_nn(windows_size, word_vec_size, hidden_layer_nodes)\n",
    "    -- Add NN Layers\n",
    "    local net = nn.Sequential()\n",
    "    net:add(nn.Linear(window_size * word_vec_size, hidden_layer_nodes))\n",
    "    net:add(nn.Sigmoid())\n",
    "    net:add(nn.Linear(hidden_layer_nodes, 1))\n",
    "    net:add(nn.LogSoftMax())\n",
    "    -- Define Loss Function\n",
    "    local criterion = nn.ClassNLLCriterion()\n",
    "    return net, criterion\n",
    "end\n",
    "\n",
    "function trainAndUpdatedWordVec(net, criterion, input, output)\n",
    "    for i = 1, 20 do\n",
    "        -- feed it to the neural network and the criterion\n",
    "        criterion:forward(net:forward(input), real_output)\n",
    "\n",
    "        -- train over this example in 3 steps\n",
    "        -- (1) zero the accumulation of the gradients\n",
    "        net:zeroGradParameters()\n",
    "\n",
    "        -- (2) accumulate gradients\n",
    "        net:backward(input, criterion:backward(net.output, output))\n",
    "        \n",
    "        print(net.gradInput)\n",
    "        \n",
    "        -- (3) update parameters with a 0.01 learning rate\n",
    "        net:updateParameters(0.01)\n",
    "    end\n",
    "    return data\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : \n",
       "    {\n",
       "      data : DoubleTensor - size: 125\n",
       "      label : 1\n",
       "    }\n",
       "  2 : \n",
       "    {\n",
       "      data : DoubleTensor - size: 125\n",
       "      label : 1\n",
       "    }\n",
       "  3 : \n",
       "    {\n",
       "      data : DoubleTensor - size: 125\n",
       "      label : 1\n",
       "    }\n",
       "  4 : \n",
       "    {\n",
       "      data : DoubleTensor - size: 125\n",
       "      label : 1\n",
       "    }\n",
       "  5 : \n",
       "    {\n",
       "      data : DoubleTensor - size: 125\n",
       "      label : 1\n",
       "    }\n",
       "  6 : \n",
       "    {\n",
       "      data : DoubleTensor - size: 125\n",
       "      label : 1\n",
       "    }\n",
       "  7 : \n",
       "    {\n",
       "      data : DoubleTensor - size: 125\n",
       "      label : 1\n",
       "    }\n",
       "  8 : \n",
       "    {\n",
       "      data : DoubleTensor - size: 125\n",
       "      label : 1\n",
       "    }\n",
       "  9 : \n",
       "    {\n",
       "      data : DoubleTensor - size: 125\n",
       "      label : 1\n",
       "    }\n",
       "  10 : \n",
       "    {\n",
       "      data : DoubleTensor - size: 125\n",
       "      "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "label : 1\n",
       "    }\n",
       "  11 : \n",
       "    {\n",
       "      data : DoubleTensor - size: 125\n",
       "      label : 1\n",
       "    }\n",
       "}\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store, word_dict = readFile(FILE_PATH)\n",
    "train_data = makeTrainingData(store, WINDOW_SIZE, word_dict)\n",
    "print(train_data)\n",
    "--net, crit = construct_nn(WINDOWS_SIZE, WORD_VEC_SIZE, 50)\n",
    "--for data, label in ipairs(train_data) do\n",
    "    --data = trainAndUpdatedWordVec(net, crit, data, label)\n",
    "--end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
