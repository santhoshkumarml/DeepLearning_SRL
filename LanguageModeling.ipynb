{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "require 'torch';\n",
    "require 'nn';\n",
    "\n",
    "START = \"$START$\"\n",
    "FINISH = \"$END$\"\n",
    "FILE_PATH = \"dataset/language_model.data\"\n",
    "WORD_VEC_SIZE = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "-- read and Store Data\n",
    "function readFile(file_path)\n",
    "    local f = io.open(file_path)\n",
    "    store = {}\n",
    "    word_dict = {}\n",
    "    while true do\n",
    "        local l = f:read()\n",
    "        if not l then break end\n",
    "        words = {}\n",
    "        table.insert(words, START)\n",
    "        for word in l:gmatch(\"%w+\") do \n",
    "            table.insert(words, word) \n",
    "            word_dict[word] = true\n",
    "        end\n",
    "        table.insert(words,FINISH)\n",
    "        table.insert(store, words)\n",
    "    end\n",
    "    return store, word_dict\n",
    "end\n",
    "\n",
    "-- split into windows and make training data\n",
    "function makeTrainingData(store, window_size)\n",
    "    train_data = {}\n",
    "    for idx, line in ipairs(store) do\n",
    "        size = table.getn(line)\n",
    "        no_of_windows = math.max( 1, size - window_size)\n",
    "\n",
    "        for widx = 1, no_of_windows do\n",
    "            training_inst = {}\n",
    "            data = {}\n",
    "            word_idx = widx+1\n",
    "            \n",
    "            add_neg_inst = false\n",
    "\n",
    "            while table.getn(data) < window_size and word_idx < size do\n",
    "                table.insert(data, line[word_idx])\n",
    "                word_idx = word_idx + 1\n",
    "            end\n",
    "            \n",
    "            while word_idx >= size and table.getn(data) < window_size do\n",
    "                table.insert(data, FINISH)\n",
    "                word_idx = word_idx + 1\n",
    "            end\n",
    "            \n",
    "            training_inst.data = data\n",
    "            training_inst.label = 1\n",
    "            table.insert(train_data, training_inst)\n",
    "        end   \n",
    "    end\n",
    "    return train_data\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "[string \"function construct_nn(windows_size, word_vec_...\"]:33: 'end' expected (to close 'function' at line 12) near '<eof>'",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "[string \"function construct_nn(windows_size, word_vec_...\"]:33: 'end' expected (to close 'function' at line 12) near '<eof>'"
     ]
    }
   ],
   "source": [
    "function construct_nn(windows_size, word_vec_size, hidden_layer_nodes)\n",
    "    -- Add NN Layers\n",
    "    net = nn.Sequential()\n",
    "    net:add(nn.Linear(window_size * word_vec_size, hidden_layer_nodes))\n",
    "    net:add(nn.Sigmoid())\n",
    "    net:add(nn.Linear(hidden_layer_nodes, 1))\n",
    "    net:add(nn.LogSoftMax())\n",
    "    -- Define Loss Function\n",
    "    criterion = nn.ClassNLLCriterion()\n",
    "    return net, criterion\n",
    "end\n",
    "\n",
    "function trainAndUpdatedWordVec(net, criterion, input, output)\n",
    "    for i = 1, 20 do\n",
    "        -- feed it to the neural network and the criterion\n",
    "        criterion:forward(net:forward(input), real_output)\n",
    "\n",
    "        -- train over this example in 3 steps\n",
    "        -- (1) zero the accumulation of the gradients\n",
    "        net:zeroGradParameters()\n",
    "\n",
    "        -- (2) accumulate gradients\n",
    "        net:backward(input, criterion:backward(net.output, output))\n",
    "        \n",
    "        grad_ip = net.gradInput\n",
    "        \n",
    "        print(grad_ip)\n",
    "                \n",
    "        -- (3) update parameters with a 0.01 learning rate\n",
    "        net:updateParameters(0.01)\n",
    "    end\n",
    "    return data\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store, word_dict = readFile(FILE_PATH)\n",
    "train_data = makeTrainingData(store, WINDOWS_SIZE)\n",
    "net, crit = construct_nn(WINDOWS_SIZE, WORD_VEC_SIZE, 50)\n",
    "for data, label in ipairs(train_data) do\n",
    "    data = trainAndUpdatedWordVec(net, crit, data, label)\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
